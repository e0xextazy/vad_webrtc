# -*- coding: utf-8 -*-
"""doMusicAndSpeechDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11Dht5YyrEEecIiqH5rYgJCLRZ9LzgCMF
"""

import soundfile as sf
import argparse
import numpy as np
import librosa
import tensorflow as tf
import math
from tensorflow import keras
from tensorflow.keras import layers


import asyncio
import concurrent.futures
import json
import logging
import os

import websockets

import os
from flask import Flask, flash, request, redirect, url_for, jsonify

from werkzeug.utils import secure_filename

UPLOAD_FOLDER = 'uploads'
ALLOWED_EXTENSIONS = {'wav','WAV'}

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER


# vad_interface = os.environ.get('VAD_SERVER_INTERFACE', '0.0.0.0')
# vad_port = int(os.environ.get('VAD_SERVER_PORT', 2700))
# vad_model_path = os.environ.get('VAD_MODEL_PATH', 'model')
# vad_sample_rate = float(os.environ.get('VAD_SAMPLE_RATE', 8000))

# pool = concurrent.futures.ThreadPoolExecutor((os.cpu_count() or 1))
# loop = asyncio.get_event_loop()

# class Model(object):
#     def __init__(self, *args):
#         nothing = False
#         self._res = []

#     def AcceptWaveform(self, data):
#         # self._res.append(vad(data))
#         self._res.append([[0],[10]])
#         return True

#     def Result(self):
#         return {"vad":self._res}

#     def PartialResult(self):
#         return {"vad":self._res}

#     def FinalResult(self):
#         return {"vad":self._res}


# def process_chunk(rec, message):
#     if message == '{"eof" : 1}':
#         return rec.FinalResult(), True
#     elif rec.AcceptWaveform(message):
#         return rec.Result(), False
#     else:
#         return rec.PartialResult(), False

# async def recognize(websocket, path):

#     rec = None
#     phrase_list = None
#     sample_rate = vad_sample_rate

#     while True:

#         message = await websocket.recv()

#         # Load configuration if provided
#         if isinstance(message, str) and 'config' in message:
#             jobj = json.loads(message)['config']
#             if 'phrase_list' in jobj:
#                 phrase_list = jobj['phrase_list']
#             if 'sample_rate' in jobj:
#                 sample_rate = float(jobj['sample_rate'])
#             continue

#         # Create the recognizer, word list is temporary disabled since not every model supports it
#         if not rec:
#             if phrase_list:
#                 #see = recognize(test_audio)
#                  rec = Model() #KaldiRecognizer(model, sample_rate, json.dumps(phrase_list))
#             else:
#                  rec = Model() #KaldiRecognizer(model, sample_rate)

#         response, stop = await loop.run_in_executor(pool, process_chunk, rec, message)
#         await websocket.send(response)
#         if stop: break



"""
This function converts the predictions made by the neural network into a readable format.
"""

def preds_to_se_json(p, audio_clip_length = 8.0):
  start_speech = -100
  start_music = -100
  stop_speech = -100
  stop_music = -100
  frames_to_time_milli = lambda x: np.round(frames_to_time(x)*1000)

  audio_events = []

  n_frames = p.shape[0]

  if p[0, 0] == 1:
    start_speech = 0
  
  if p[0, 1] == 1:
    start_music = 0

  for i in range(n_frames - 1):
    if p[i, 0] == 0 and p[i + 1, 0] == 1:
      start_speech = i + 1

    elif p[i, 0] == 1 and p[i + 1, 0] == 0:
      stop_speech = i
      start_time = frames_to_time_milli(start_speech)
      stop_time = frames_to_time_milli(stop_speech)
      audio_events.append({"type":"speech","start_time":start_time, "end_time":stop_time})
      start_speech = -100
      stop_speech = -100

    if p[i, 1] == 0 and p[i + 1, 1] == 1:
      start_music = i + 1
    elif p[i, 1] == 1 and p[i + 1, 1] == 0:
      stop_music = i
      start_time = frames_to_time_milli(start_music)
      stop_time = frames_to_time_milli(stop_music)      
      audio_events.append({"type":"music","start_time":start_time, "end_time":stop_time})
      start_music = -100
      stop_music = -100

  if start_speech != -100:
    start_time = frames_to_time_milli(start_speech)
    stop_time = np.round(audio_clip_length * 1000)
    audio_events.append({"type":"speech","start_time":start_time, "end_time":stop_time})
    start_speech = -100
    stop_speech = -100

  if start_music != -100:
    start_time = frames_to_time_milli(start_music)
    stop_time = np.round(audio_clip_length * 1000)
    audio_events.append({"type":"music","start_time":start_time, "end_time":stop_time})
    start_music = -100
    stop_music = -100

  audio_events.sort(key = lambda x: x['start_time']) 
  return audio_events

def preds_to_se(p, audio_clip_length = 8.0):
  start_speech = -100
  start_music = -100
  stop_speech = -100
  stop_music = -100

  audio_events = []

  n_frames = p.shape[0]

  if p[0, 0] == 1:
    start_speech = 0
  
  if p[0, 1] == 1:
    start_music = 0

  for i in range(n_frames - 1):
    if p[i, 0] == 0 and p[i + 1, 0] == 1:
      start_speech = i + 1

    elif p[i, 0] == 1 and p[i + 1, 0] == 0:
      stop_speech = i
      start_time = frames_to_time(start_speech)
      stop_time = frames_to_time(stop_speech)
      audio_events.append((start_time, stop_time, "speech"))
      start_speech = -100
      stop_speech = -100

    if p[i, 1] == 0 and p[i + 1, 1] == 1:
      start_music = i + 1
    elif p[i, 1] == 1 and p[i + 1, 1] == 0:
      stop_music = i
      start_time = frames_to_time(start_music)
      stop_time = frames_to_time(stop_music)      
      audio_events.append((start_time, stop_time, "music"))
      start_music = -100
      stop_music = -100

  if start_speech != -100:
    start_time = frames_to_time(start_speech)
    stop_time = audio_clip_length
    audio_events.append((start_time, stop_time, "speech"))
    start_speech = -100
    stop_speech = -100

  if start_music != -100:
    start_time = frames_to_time(start_music)
    stop_time = audio_clip_length
    audio_events.append((start_time, stop_time, "music"))
    start_music = -100
    stop_music = -100

  audio_events.sort(key = lambda x: x[0]) 
  return audio_events



""" This function was adapted from https://github.com/qlemaire22/speech-music-detection """

def smooth_output(output, min_speech=1.3, min_music=3.4, max_silence_speech=0.4, max_silence_music=0.6):
    # This function was adapted from https://github.com/qlemaire22/speech-music-detection
    duration_frame = 220 / 22050
    n_frame = output.shape[1]

    start_music = -1000
    start_speech = -1000

    for i in range(n_frame):
        if output[0, i] == 1:
            if i - start_speech > 1:
                if (i - start_speech) * duration_frame <= max_silence_speech:
                    output[0, start_speech:i] = 1

            start_speech = i

        if output[1, i] == 1:
            if i - start_music > 1:
                if (i - start_music) * duration_frame <= max_silence_music:
                    output[1, start_music:i] = 1

            start_music = i

    start_music = -1000
    start_speech = -1000

    for i in range(n_frame):
        if i != n_frame - 1:
            if output[0, i] == 0:
                if i - start_speech > 1:
                    if (i - start_speech) * duration_frame <= min_speech:
                        output[0, start_speech:i] = 0

                start_speech = i

            if output[1, i] == 0:
                if i - start_music > 1:
                    if (i - start_music) * duration_frame <= min_music:
                        output[1, start_music:i] = 0

                start_music = i
        else:
            if i - start_speech > 1:
                if (i - start_speech) * duration_frame <= min_speech:
                    output[0, start_speech:i + 1] = 0

            if i - start_music > 1:
                if (i - start_music) * duration_frame <= min_music:
                    output[1, start_music:i + 1] = 0

    return output


def frames_to_time(f, sr = 22050.0, hop_size = 220):
  return f * hop_size / sr

def get_log_melspectrogram(audio, sr = 22050, hop_length = 220, n_fft = 1024, n_mels = 80, fmin = 64, fmax = 8000):
    """Return the log-scaled Mel bands of an audio signal."""
    bands = librosa.feature.melspectrogram(
        y=audio, sr=sr, hop_length=hop_length, n_fft=n_fft, n_mels=n_mels, fmin=fmin, fmax=fmax, dtype=np.float32)
    return librosa.core.power_to_db(bands, amin=1e-7)


"""
Make predictions for full audio.
"""
def mk_preds_fa(audio_path, hop_size = 6.0, discard = 1.0, win_length = 8.0, sampling_rate = 22050):
#   in_sr = 8000
  in_signal, in_sr = sf.read(audio_path)
#   in_signal, in_sr = audio_path

  # Convert to mono if needed.
  if (in_signal.ndim > 1):
    in_signal_mono = librosa.to_mono(in_signal.T)
    in_signal = np.copy(in_signal_mono)
  # Resample the audio file.
  in_signal_22k = librosa.resample(in_signal, orig_sr=in_sr, target_sr=sampling_rate)
  in_signal = np.copy(in_signal_22k)

  # Pad the input signal if it is shorter than 8 s.

  if in_signal.shape[0] < int(8.0 * sampling_rate):
  	pad_signal = np.zeros((int(8.0 * sampling_rate)))
  	pad_signal[:in_signal.shape[0]] = in_signal
  	in_signal = np.copy(pad_signal)

  audio_clip_length_samples = in_signal.shape[0]
  print('audio_clip_length_samples is {}'.format(audio_clip_length_samples))

  hop_size_samples = 220 * 602 - 1

  win_length_samples = 220 * 802 - 1

  n_preds = int(math.ceil((audio_clip_length_samples - win_length_samples) / hop_size_samples)) + 1

  in_signal_pad = np.zeros((n_preds * hop_size_samples + 200 * 220))

  in_signal_pad[0:audio_clip_length_samples] = in_signal

  preds = np.zeros((n_preds, 802, 2))

  # Split the predictions into batches of size batch_size. 

  batch_size = 128

  n_batch = n_preds // batch_size

  for i in range(n_batch):
    mss_batch = np.zeros((batch_size, 802, 80))
    for j in range(batch_size):
      seg = in_signal_pad[(i * batch_size + j)* hop_size_samples:((i * batch_size + j) * hop_size_samples) + win_length_samples]
      seg = librosa.util.normalize(seg)
      mss = get_log_melspectrogram(seg)
      M = mss.T
      mss_batch[j, :, :] = M

    preds[i * batch_size:(i + 1) * batch_size, :, :] = (model.predict(mss_batch) >= (0.5, 0.5)).astype(np.float)

  if n_batch * batch_size < n_preds:
    i = n_batch
    mss_batch = np.zeros((n_preds - n_batch * batch_size, 802, 80))
    for j in range(n_preds - n_batch * batch_size):
      seg = in_signal_pad[(i * batch_size + j)* hop_size_samples:((i * batch_size + j) * hop_size_samples) + win_length_samples]
      seg = librosa.util.normalize(seg)
      mss = get_log_melspectrogram(seg)
      M = mss.T
      mss_batch[j, :, :] = M

    preds[i * batch_size:n_preds, :, :] = (model.predict(mss_batch) >= (0.5, 0.5)).astype(np.float)

  preds_mid = np.copy(preds[1:-1, 100:702, :])

  preds_mid_2 = preds_mid.reshape(-1, 2)

  if preds.shape[0] > 1:
    oa_preds = preds[0, 0:702, :] # oa stands for overall predictions

  else:
    oa_preds = preds[0, 0:802, :] # oa stands for overall predictions

  oa_preds = np.concatenate((oa_preds, preds_mid_2), axis = 0)

  if preds.shape[0] > 1:
    oa_preds = np.concatenate((oa_preds, preds[-1, 100:, :]), axis = 0)

  return oa_preds

def vad(test_audio):
    ss, _ = sf.read(test_audio)
    # ss = test_audio
    oop = mk_preds_fa(test_audio)

    p_smooth = smooth_output(oop.T, min_speech=1.3, min_music=3.4, max_silence_speech=0.4, max_silence_music=0.6)
    p_smooth = p_smooth.T
    see = preds_to_se_json(p_smooth, audio_clip_length=ss.shape[0]/22050.0)
    return see

def allowed_file(filename):
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

@app.route('/file-upload', methods=['POST'])
def upload_file():
    if 'wav' not in request.files:
        resp = jsonify({'message' : 'No file part in the request'})
        resp.status_code = 400
        return resp
    file = request.files['wav']
    data = request.files['json']
    # data = data.read()
    if file.filename == '':
        resp = jsonify({'message' : 'No file selected for uploading'})
        resp.status_code = 400
        return resp
    if file and allowed_file(file.filename):
        filename = secure_filename(file.filename)
        dataname = secure_filename(data.filename)
        # data.save(os.path.join(app.config['UPLOAD_FOLDER'], dataname))
        file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))
        # f = file.read()
        # message = json.loads(data)
        vdata = {"mvad":vad(os.path.join(app.config['UPLOAD_FOLDER'], filename))}
        jdata = json.loads(data.read())
        resp = jsonify({**vdata,**jdata})
        resp.status_code = 201
        return resp
    else:
        resp = jsonify({'message' : 'Allowed file types are txt, pdf, png, jpg, jpeg, gif'})
        resp.status_code = 400
        return resp

if __name__ == '__main__':

  parser = argparse.ArgumentParser(description="Music and speech detection on a given audio and output as txt file")
  parser.add_argument('input_path', help='Input wav file path')
  parser.add_argument('output_path', help="Output txt file path")

#   args = parser.parse_args()

#   test_audio = args.input_path

  m = 'model d-DS.h5'

  mel_input = keras.Input(shape=(802, 80), name="mel_input")
  X = mel_input

  X = tf.keras.layers.Reshape((802, 80, 1))(X)

  X = tf.keras.layers.Conv2D(filters=16, kernel_size=7, strides=1, padding='same')(X)
  X = layers.BatchNormalization(momentum=0.0)(X)
  X = tf.keras.layers.Activation('relu')(X)
  X = tf.keras.layers.MaxPool2D(pool_size=(1, 2))(X)
  X = tf.keras.layers.Dropout(rate = 0.2)(X)

  X = tf.keras.layers.Conv2D(filters=64, kernel_size=7, strides=1, padding='same')(X)
  X = layers.BatchNormalization(momentum=0.0)(X)
  X = tf.keras.layers.Activation('relu')(X)
  X = tf.keras.layers.MaxPool2D(pool_size=(1, 2))(X)
  X = layers.Dropout(rate = 0.2)(X)

  _, _, sx, sy = X.shape
  X = tf.keras.layers.Reshape((-1, int(sx * sy)))(X)

  X = layers.Bidirectional(layers.GRU(80, return_sequences = True))(X)
  X = layers.BatchNormalization(momentum=0.0)(X)

  X = layers.Bidirectional(layers.GRU(80, return_sequences = True))(X)
  X = layers.BatchNormalization(momentum=0.0)(X)

  pred = layers.TimeDistributed(layers.Dense(2, activation='sigmoid'))(X)

  model = keras.Model(inputs = [mel_input], outputs = [pred])

  model.compile(
      optimizer=keras.optimizers.Adam(learning_rate=0.001),
      loss=[keras.losses.BinaryCrossentropy()], metrics=['binary_accuracy'])

  # model.summary()

  model.load_weights(m)
  
  app.run(port=5001)
#   start_server = websockets.serve(
#   recognize, vad_interface, vad_port)

#   loop.run_until_complete(start_server)
#   loop.run_forever()

#   test_audio = 'audio\\1.wav'
#   see = recognize(test_audio)
#   ss, _ = sf.read(test_audio)
#   oop = mk_preds_fa(test_audio)

#   p_smooth = smooth_output(oop.T, min_speech=1.3, min_music=3.4, max_silence_speech=0.4, max_silence_music=0.6)
#   p_smooth = p_smooth.T
#   see = preds_to_se(p_smooth, audio_clip_length=ss.shape[0]/22050.0)

#   n_label = args.output_path

#   with open(n_label, 'w') as fp:
#     fp.write('\n'.join('{}\t{}\t{}'.format(round(x[0], 5), round(x[1], 5), x[2]) for x in see))
